{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import PassiveAggressiveClassifier, SGDClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from skmultiflow.trees import HoeffdingTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kartik Gounder\\Desktop\\Projects\\.venv\\Lib\\site-packages\\skmultiflow\\trees\\hoeffding_tree.py:32: FutureWarning: 'HoeffdingTree' has been renamed to 'HoeffdingTreeClassifier' in v0.5.0.\n",
      "The old name will be removed in v0.7.0\n",
      "  warnings.warn(\"'HoeffdingTree' has been renamed to 'HoeffdingTreeClassifier' in v0.5.0.\\n\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Generate synthetic data for training\n",
    "num_samples = 1000\n",
    "X_train_synthetic = np.random.randn(num_samples, 10)\n",
    "y_train_synthetic = np.random.randint(0, 2, num_samples)\n",
    "\n",
    "# Initialize classifiers\n",
    "pac = PassiveAggressiveClassifier(random_state=42)\n",
    "sgd = SGDClassifier(random_state=42)\n",
    "gb = GradientBoostingClassifier(random_state=42)\n",
    "ht = HoeffdingTree()\n",
    "\n",
    "classifiers = [pac, sgd, gb, ht]\n",
    "classifier_names = ['PAC', 'SDG', 'GradientBoosting', 'HoeffdingTree']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GradientBoostingClassifier' object has no attribute 'partial_fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_samples):\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m clf, name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(classifiers, classifier_names):\n\u001b[1;32m----> 8\u001b[0m         \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartial_fit\u001b[49m(X_train_synthetic[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], y_train_synthetic[i:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], classes\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39munique(y_train_synthetic))\n\u001b[0;32m      9\u001b[0m         y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_train_synthetic[:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     10\u001b[0m         acc \u001b[38;5;241m=\u001b[39m accuracy_score(y_train_synthetic[:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m], y_pred)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'GradientBoostingClassifier' object has no attribute 'partial_fit'"
     ]
    }
   ],
   "source": [
    "\n",
    "# Track accuracy during training\n",
    "accuracies = {name: [] for name in classifier_names}\n",
    "ensemble_accuracies = []\n",
    "\n",
    "# Train classifiers incrementally and track accuracy\n",
    "for i in range(num_samples):\n",
    "    for clf, name in zip(classifiers, classifier_names):\n",
    "        clf.partial_fit(X_train_synthetic[i:i+1], y_train_synthetic[i:i+1], classes=np.unique(y_train_synthetic))\n",
    "        y_pred = clf.predict(X_train_synthetic[:i+1])\n",
    "        acc = accuracy_score(y_train_synthetic[:i+1], y_pred)\n",
    "        accuracies[name].append(acc)\n",
    "    \n",
    "    # Ensemble prediction\n",
    "    ensemble_pred = np.mean([clf.predict(X_train_synthetic[i:i+1]) for clf in classifiers])\n",
    "    ensemble_acc = accuracy_score(y_train_synthetic[:i+1], (ensemble_pred >= 0.5).astype(int))\n",
    "    ensemble_accuracies.append(ensemble_acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "for name in classifier_names:\n",
    "    plt.plot(np.arange(num_samples), accuracies[name], label=name)\n",
    "\n",
    "plt.plot(np.arange(num_samples), ensemble_accuracies, label='Ensemble', linestyle='--', color='black')\n",
    "\n",
    "# Plot target accuracy points\n",
    "plt.axhline(y=0.9935, color='r', linestyle='-.', label='PAC Target')\n",
    "plt.axhline(y=0.9778, color='g', linestyle='-.', label='SDG Target')\n",
    "plt.axhline(y=0.9918, color='b', linestyle='-.', label='Hoeffding Target')\n",
    "plt.axhline(y=0.9953, color='m', linestyle='-.', label='Ensemble Target')\n",
    "\n",
    "plt.title('Learning Curve: Accuracy vs. Training Samples')\n",
    "plt.xlabel('Number of Training Samples')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
