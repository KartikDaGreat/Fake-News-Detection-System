{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.linear_model import PassiveAggressiveClassifier\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# from collections import defaultdict\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import spacy\n",
    "from datetime import datetime\n",
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GLOBAL VARIABLE DEFINITION:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "global global_list\n",
    "global_list = [ \n",
    "    [\n",
    "        ['keyword1', 'keyword2', 'keyword3'],\n",
    "        [\n",
    "            ['January 1, 2022', 'Text 1'],\n",
    "            ['January 3, 2022', 'Text 2'],\n",
    "            ['December 18, 2021', 'Text 3']\n",
    "        ]\n",
    "    ],\n",
    "    [\n",
    "        ['keyword4', 'keyword5', 'keyword6'],\n",
    "        [\n",
    "            ['January 10, 2022', 'Text 4'],\n",
    "            ['January 30, 2022', 'Text 5'],\n",
    "            ['April 12, 20221', 'Text 6']\n",
    "        ]\n",
    "    ]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All functions combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_keywords(text, num_keywords=5):\n",
    "    # Tokenize the text\n",
    "    tokenizer = RegexpTokenizer(r'\\b[a-zA-Z]{3,}\\b')  # Keep only words with 3 or more characters\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    # Remove stopwords and lemmatize the tokens\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    filtered_tokens = [lemmatizer.lemmatize(word.lower()) for word in tokens if word.lower() not in stop_words]\n",
    "    # Calculate word frequency\n",
    "    freq_dist = FreqDist(filtered_tokens)\n",
    "    # Get the most frequent non-stopword lemmatized words\n",
    "    top_keywords = freq_dist.most_common(num_keywords)\n",
    "    return [word for word, _ in top_keywords]\n",
    "\n",
    "\n",
    "def most_similar_chain(input_keywords):\n",
    "    max_similarity = 0\n",
    "    pos = -1\n",
    "    temp = -1\n",
    "    for chain in global_list:  # Iterate over each chain in global_list\n",
    "        temp += 1\n",
    "        keywords = chain[0]  # Keywords are in the first element of the chain\n",
    "        chain_items = chain[1]  # Text-date pairs are in the second element of the chain\n",
    "        if not chain_items:  # Skip empty chains\n",
    "            continue\n",
    "        # Calculate similarity with keywords\n",
    "        keyword_similarity = sum(nlp(keyword).similarity(nlp(input_keyword)) for keyword in keywords for input_keyword in input_keywords)\n",
    "        # Calculate similarity with text in each text-date pair\n",
    "        text_similarity = sum(nlp(text).similarity(nlp(input_keyword)) for date, text in chain_items for input_keyword in input_keywords for text in [text_date_pair[1]])\n",
    "        # Calculate overall similarity as the sum of keyword similarity and text similarity\n",
    "        similarity = (keyword_similarity + text_similarity) / (len(input_keywords) + len(chain_items))\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            pos = temp\n",
    "    if max_similarity < 0.5:\n",
    "        pos = -1\n",
    "    return pos\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def chain_item_sorting(list_of_lists):\n",
    "    # Sort the list of lists based on the date in each inner list\n",
    "    temp_list = list_of_lists[1]\n",
    "    sorted_list = sorted(list_of_lists[1:], key=lambda x: datetime.strptime(x[0][0], \"%B %d, %Y\"))\n",
    "    list_of_lists[1] = sorted_list\n",
    "    return\n",
    "\n",
    "# def sort_by_date(date_text_pairs):\n",
    "#     # Convert the date strings to datetime objects\n",
    "#     for pair in date_text_pairs:\n",
    "#         pair[0] = datetime.strptime(pair[0], '%B %d, %Y')\n",
    "#     # Sort the list based on the datetime objects\n",
    "#     sorted_pairs = sorted(date_text_pairs, key=lambda x: x[0])\n",
    "#     # Convert the datetime objects back to strings\n",
    "#     for pair in sorted_pairs:\n",
    "#         pair[0] = datetime.strftime(pair[0], '%B %d, %Y')\n",
    "#     return sorted_pairs\n",
    "\n",
    "\n",
    "\n",
    "def add_to_global_var(text_input, date_added):\n",
    "    kw_list = extract_keywords(text_input)\n",
    "    pos = most_similar_chain(kw_list)\n",
    "    if pos == -1:\n",
    "        global_list.append([kw_list, [[date_added, text_input]]])\n",
    "        chain_item_sorting(global_list[-1])\n",
    "    else:\n",
    "        global_list[pos].append([date_added, text_input])\n",
    "        chain_item_sorting(global_list[pos])\n",
    "\n",
    "\n",
    "def print_global_list():\n",
    "    for chain in global_list:\n",
    "        print(\"Keywords:\", chain[0])\n",
    "        print(\"Text-Date Pairs:\")\n",
    "        for pair in chain[1]:\n",
    "            print(\"  Text:\", pair)\n",
    "            print()\n",
    "        print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Sample fake news text\n",
    "# fake_news_text = \"\"\"\n",
    "# Breaking: Scientists confirm aliens exist and have made contact with Earth!\n",
    "# According to reliable sources, extraterrestrial beings have been communicating with top government officials for years. The evidence suggests that they have advanced technology far beyond our understanding.\n",
    "# \"\"\"\n",
    "\n",
    "# # Extract keywords\n",
    "# keywords = extract_keywords(fake_news_text)\n",
    "# print(\"Keywords for fake news detection:\", keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to find the most similar chain with input of list of keywords:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "\n",
    "\n",
    "# def most_similar_chain(input_keywords):\n",
    "#     max_similarity = 0\n",
    "#     most_similar_chain = None\n",
    "\n",
    "#     for item in global_list:  # Skip the first item which contains only keywords\n",
    "#         keywords = item[0]  # Keywords are in the first element of each item\n",
    "#         similarity = sum(nlp(keyword).similarity(nlp(input_keyword)) for keyword in keywords for input_keyword in input_keywords)\n",
    "#         if similarity > max_similarity:\n",
    "#             max_similarity = similarity\n",
    "#             most_similar_chain = item\n",
    "#     max_similarity = max_similarity/len(input_keywords)\n",
    "#     if max_similarity<0.5:\n",
    "#         return []\n",
    "#     print(max_similarity)\n",
    "#     print(\": is the maximum similarity\")\n",
    "#     return most_similar_chain\n",
    "\n",
    "# # Example usage\n",
    "\n",
    "\n",
    "# input_keywords = ['keyword1', 'keyword4', 'keyword5', 'keyword7', 'keyword8', 'keyword9']\n",
    "\n",
    "# most_similar_chain = most_similar_chain(input_keywords)\n",
    "# print(\"Most similar chain:\", most_similar_chain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date based sorting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def chain_item_sorting(list_of_lists):\n",
    "#     # Sort the list of lists based on the date in each inner list\n",
    "#     sorted_list = sorted(list_of_lists[1:], key=lambda x: datetime.strptime(x['date'], \"%B %d, %Y\"))\n",
    "\n",
    "#     # Combine the sorted list with the first item (keywords)\n",
    "#     result = [list_of_lists[0]] + sorted_list\n",
    "\n",
    "#     return result\n",
    "\n",
    "# # Example usage\n",
    "# list_of_lists = [\n",
    "#     ['keyword1', 'keyword2', 'keyword3'],\n",
    "#     ['January 1, 2022', 'Text 1'],\n",
    "#     ['January 3, 2022', 'Text 2'],\n",
    "#     ['December 18, 2021', 'Text 3']\n",
    "# ]\n",
    "\n",
    "# sorted_list_of_lists = chain_item_sorting(list_of_lists)\n",
    "# print(sorted_list_of_lists)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kartik Gounder\\AppData\\Local\\Temp\\ipykernel_6444\\955989383.py:27: UserWarning: [W008] Evaluating Doc.similarity based on empty vectors.\n",
      "  keyword_similarity = sum(nlp(keyword).similarity(nlp(input_keyword)) for keyword in keywords for input_keyword in input_keywords)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'text_date_pair' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m sample_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhello world keyword1 keyword2 keyword3\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m sample_date \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJanuary 3, 2020\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43madd_to_global_var\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43msample_date\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 65\u001b[0m, in \u001b[0;36madd_to_global_var\u001b[1;34m(text_input, date_added)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_to_global_var\u001b[39m(text_input, date_added):\n\u001b[0;32m     64\u001b[0m     kw_list \u001b[38;5;241m=\u001b[39m extract_keywords(text_input)\n\u001b[1;32m---> 65\u001b[0m     pos \u001b[38;5;241m=\u001b[39m \u001b[43mmost_similar_chain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkw_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pos \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m     67\u001b[0m         global_list\u001b[38;5;241m.\u001b[39mappend([kw_list, [[date_added, text_input]]])\n",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m, in \u001b[0;36mmost_similar_chain\u001b[1;34m(input_keywords)\u001b[0m\n\u001b[0;32m     27\u001b[0m keyword_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nlp(keyword)\u001b[38;5;241m.\u001b[39msimilarity(nlp(input_keyword)) \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords \u001b[38;5;28;01mfor\u001b[39;00m input_keyword \u001b[38;5;129;01min\u001b[39;00m input_keywords)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate similarity with text in each text-date pair\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m text_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_keyword\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchain_items\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_keyword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minput_keywords\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_date_pair\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate overall similarity as the sum of keyword similarity and text similarity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m similarity \u001b[38;5;241m=\u001b[39m (keyword_similarity \u001b[38;5;241m+\u001b[39m text_similarity) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(input_keywords) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chain_items))\n",
      "Cell \u001b[1;32mIn[3], line 29\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     27\u001b[0m keyword_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nlp(keyword)\u001b[38;5;241m.\u001b[39msimilarity(nlp(input_keyword)) \u001b[38;5;28;01mfor\u001b[39;00m keyword \u001b[38;5;129;01min\u001b[39;00m keywords \u001b[38;5;28;01mfor\u001b[39;00m input_keyword \u001b[38;5;129;01min\u001b[39;00m input_keywords)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Calculate similarity with text in each text-date pair\u001b[39;00m\n\u001b[1;32m---> 29\u001b[0m text_similarity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(nlp(text)\u001b[38;5;241m.\u001b[39msimilarity(nlp(input_keyword)) \u001b[38;5;28;01mfor\u001b[39;00m date, text \u001b[38;5;129;01min\u001b[39;00m chain_items \u001b[38;5;28;01mfor\u001b[39;00m input_keyword \u001b[38;5;129;01min\u001b[39;00m input_keywords \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m [\u001b[43mtext_date_pair\u001b[49m[\u001b[38;5;241m1\u001b[39m]])\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate overall similarity as the sum of keyword similarity and text similarity\u001b[39;00m\n\u001b[0;32m     31\u001b[0m similarity \u001b[38;5;241m=\u001b[39m (keyword_similarity \u001b[38;5;241m+\u001b[39m text_similarity) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;28mlen\u001b[39m(input_keywords) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(chain_items))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text_date_pair' is not defined"
     ]
    }
   ],
   "source": [
    "sample_text = \"hello world keyword1 keyword2 keyword3\"\n",
    "sample_date = \"January 3, 2020\"\n",
    "add_to_global_var(sample_text,sample_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text1 = \"hello sample world keyword1 keyword2 keyword3\"\n",
    "sample_date1 = \"April 10, 2024\"\n",
    "add_to_global_var(sample_text1,sample_date1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_global_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
